# RAG-Based Enterprise Knowledge Assistant (n8n)

A modular, privacy-conscious internal AI assistant that helps employees quickly retrieve information from company documents and data ‚Äî powered by **n8n**, **local or cloud LLMs**, and **vector search**. This project is designed to be deployable even in **offline or restricted environments**.

---

## üß† Features

- üìé **Document Ingestion & Chunking** from Google Drive, Dropbox, or local folders
- üìö **Embeddings & Indexing** via Pinecone, FAISS, or Qdrant
- üîç **Smart Query Routing** to detect summarization vs. calculation
- üìä **On-the-fly Python/Pandas Calculations** for numeric queries
- üßæ **Source-Cited Responses** grounded in real company data
- üí¨ **Slack / Email / API Integration** to query assistant easily
- üîí **Offline Mode Support** using local LLMs + vector DBs

---

## üß∞ Tech Stack

| Component | Options / Tools |
|----------|-----------------|
| Vector DB | Pinecone (cloud), FAISS/Qdrant (local) |
| Embeddings | OpenAI, HuggingFace, Ollama-compatible local models |
| LLM | GPT-4, GPT-3.5, Mistral, Phi (via Ollama) |
| Automation | [n8n](https://n8n.io/) Workflows |
| Optional Compute | Python via Pandas or external command |

---

## üì¶ Repository Structure

```bash
/n8n-enterprise-assistant/
‚îú‚îÄ‚îÄ README.md                # You're here!
‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.n8n.json   # Watches Drive, chunks, embeds, indexes
‚îÇ   ‚îú‚îÄ‚îÄ query.n8n.json       # Handles queries, retrieval, and response
‚îÇ   ‚îî‚îÄ‚îÄ sync.n8n.json        # Optional cron-based reindexing
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ base_prompt.txt      # Assistant instructions to LLM
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ pandas_runner.py     # Python code for numeric queries
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ sample_QA.md         # Realistic example queries & answers
‚îú‚îÄ‚îÄ diagrams/
‚îÇ   ‚îî‚îÄ‚îÄ system_diagram.png   # High-level workflow architecture
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ deployment_guide.md  # Setup notes and best practices
```

---

## üöÄ How It Works

### 1. Document Ingestion
- Watches a company Google Drive folder or shared directory
- Extracts file contents (PDF, DOCX, Markdown, CSV)
- Splits into overlapping chunks
- Generates embeddings and indexes them in the vector DB

### 2. Query Workflow
- Triggered via Slack, HTTP request, or email
- Detects if the query requires:
  - üîπ Summarization ‚Üí pulls relevant chunks
  - üîπ Numeric analysis ‚Üí loads spreadsheet and runs pandas
- Assembles a context block and routes it to the LLM
- Returns a clear, source-cited answer

---

## üß™ Sample Use Cases

- üßæ ‚ÄúWhat‚Äôs our PTO rollover policy?‚Äù
- üìä ‚ÄúCompare average sales from 2021 vs. 2024‚Äù
- üßº ‚ÄúList all upcoming security compliance dates‚Äù
- üì£ ‚ÄúSummarize all marketing meeting notes from Q2‚Äù

---

## üõ°Ô∏è Security / Offline Considerations

- No external API calls when using local Ollama models and FAISS
- Vector DB can run on-premise
- No document or query data leaves your local infrastructure
- Prompt includes anti-hallucination and safety instructions

---

## üìà Future Additions

- Feedback form for query satisfaction rating
- PDF export of responses for compliance
- Admin dashboard to view query patterns
- Azure AD / Google SSO login protection

---

## üì£ Want to Contribute?
This project is still under active development. Feedback, issues, or pull requests welcome!

---

## üìå Status
üß™ **Prototype complete. Ready for business testing.**

If your company handles sensitive info and wants to try AI-powered search safely ‚Äî this is for you.

> Built with ‚ù§Ô∏è using n8n, Ollama, and secure AI design practices.
